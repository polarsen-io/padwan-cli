{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Padwan CLI","text":"<p>Padwan CLI is an interactive CLI and TUI for padwan-llm, the unified LLM client library. It provides a terminal interface for querying multiple LLM providers \u2014 OpenAI, Gemini, Mistral, and Grok \u2014 through a single tool.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>One-shot queries \u2014 send a prompt and get a response, with optional streaming</li> <li>Interactive chat \u2014 multi-turn conversations with persistent session history</li> <li>Batch processing \u2014 submit, poll, and export Gemini batch jobs</li> <li>Dual interface \u2014 works as both a traditional CLI and a rich TUI (via piou)</li> <li>Multi-provider \u2014 switch between providers with a <code>-m</code> flag</li> </ul>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code># Try it without installing\nuvx padwan-cli \"Explain monads in one sentence\" -m gpt-4o-mini\n\n# One-shot query\npadwan-cli \"Explain monads in one sentence\" -m gpt-4o-mini\n\n# Stream the response\npadwan-cli \"Write a haiku about Rust\" -m gpt-4o-mini --stream\n\n# Interactive chat\npadwan-cli chat send \"Hello!\" -m gpt-4o-mini\n\n# List available models\npadwan-cli models\n</code></pre>"},{"location":"#commands-overview","title":"Commands overview","text":"Command Description (default) One-shot LLM query <code>models</code> List available models across providers <code>info</code> Show model count per provider <code>chat send</code> Start an interactive conversation <code>chat clear</code> Clear conversation history <code>batch create</code> Create a Gemini batch job <code>batch status</code> Check batch job status <code>batch list</code> List recent batch jobs <code>batch poll</code> Poll a batch job until completion <code>batch cancel</code> Cancel a batch job <code>batch retry</code> Retry failed requests from a batch <code>batch export</code> Export batch results to a file"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install padwan-cli\n</code></pre> <p>Or with uv:</p> <pre><code>uv tool install padwan-cli\n</code></pre>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.14</li> <li>API keys for the providers you want to use (set as environment variables, e.g. <code>OPENAI_API_KEY</code>, <code>GEMINI_API_KEY</code>, etc.)</li> </ul>"},{"location":"getting-started/#local-development","title":"Local development","text":"<pre><code>git clone https://github.com/Polarsen/padwan-llm.git\ngit clone https://github.com/Polarsen/padwan-cli.git\ncd padwan-cli\nuv sync --group dev\nuv run padwan-cli\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic usage","text":""},{"location":"getting-started/#one-shot-query","title":"One-shot query","text":"<p>Send a single prompt and print the response:</p> <pre><code>padwan-cli \"What is the capital of France?\" -m gpt-4o-mini\n</code></pre> <p>Use <code>--stream</code> (<code>-s</code>) to see tokens as they arrive:</p> <pre><code>padwan-cli \"Tell me a story\" -m gpt-4o-mini --stream\n</code></pre> <p>The default model is <code>gpt-4o-mini</code>. Use <code>-m</code> to pick any supported model.</p>"},{"location":"getting-started/#list-models","title":"List models","text":"<p>See all models available across providers:</p> <pre><code>padwan-cli models\n</code></pre> <p>Filter by provider:</p> <pre><code>padwan-cli models -p openai\npadwan-cli models -p gemini\n</code></pre>"},{"location":"getting-started/#library-info","title":"Library info","text":"<p>Show model counts per provider:</p> <pre><code>padwan-cli info\n</code></pre>"},{"location":"getting-started/#cli-vs-tui-mode","title":"CLI vs TUI mode","text":"<p>Padwan CLI ships with two entry points backed by the same commands:</p> <ul> <li>TUI mode (default via <code>padwan-cli</code>) \u2014 interactive terminal UI with styled output, streaming widgets, and an input prompt. Best for chat sessions and batch monitoring.</li> <li>CLI mode (<code>python -m padwan_cli</code>) \u2014 traditional stdout output. Best for scripting and piping.</li> </ul> <p>In TUI mode, chat responses render with colored messages and a live progress bar for batch jobs. In CLI mode, output is plain text suitable for piping to other tools.</p>"},{"location":"commands/batch/","title":"Batch","text":"<p>The <code>batch</code> command group manages Gemini batch prediction jobs. It lets you submit many prompts at once, monitor progress, and export results.</p> <p>Note</p> <p>Batch operations use the Gemini API exclusively. A valid <code>GEMINI_API_KEY</code> is required.</p>"},{"location":"commands/batch/#batch-create","title":"<code>batch create</code>","text":"<p>Create a new batch job from inline prompts or a file.</p> <pre><code># Inline prompts\npadwan-cli batch create -p \"Summarize quantum computing\" -p \"Explain CRISPR\"\n\n# From a JSON file\npadwan-cli batch create -f prompts.json -m gemini-2.0-flash -n my-batch\n</code></pre> Option Default Description <code>-p</code>, <code>--prompt</code> Inline prompt (can be repeated) <code>-f</code>, <code>--file</code> Path to a JSON or text file containing prompts <code>-m</code>, <code>--model</code> <code>gemini-2.0-flash</code> Model to use <code>-n</code>, <code>--name</code> <code>cli-batch</code> Display name for the batch job"},{"location":"commands/batch/#input-file-formats","title":"Input file formats","text":"JSON (string array)JSON (objects with keys)Text (one per line) <pre><code>[\n  \"What is photosynthesis?\",\n  \"Explain gravity\",\n  \"Summarize the French Revolution\"\n]\n</code></pre> <pre><code>[\n  {\"prompt\": \"What is photosynthesis?\", \"key\": \"bio-1\"},\n  {\"prompt\": \"Explain gravity\", \"key\": \"physics-1\"}\n]\n</code></pre> <pre><code>What is photosynthesis?\nExplain gravity\nSummarize the French Revolution\n</code></pre>"},{"location":"commands/batch/#batch-status","title":"<code>batch status</code>","text":"<p>Check the current status of a batch job.</p> <pre><code>padwan-cli batch status -j &lt;job-name&gt;\n\n# Include results if the job is complete\npadwan-cli batch status -j &lt;job-name&gt; -r\n</code></pre> Option Default Description <code>-j</code>, <code>--job</code> required Batch job name <code>-r</code>, <code>--results</code> <code>false</code> Show results if the job has completed"},{"location":"commands/batch/#batch-list","title":"<code>batch list</code>","text":"<p>List recent batch jobs.</p> <pre><code>padwan-cli batch list\npadwan-cli batch list -l 20\n</code></pre> Option Default Description <code>-l</code>, <code>--limit</code> <code>10</code> Maximum number of jobs to show <p>Output is a table with columns: Name, State, Display Name, Model, and Stats (succeeded/total).</p>"},{"location":"commands/batch/#batch-poll","title":"<code>batch poll</code>","text":"<p>Poll a batch job until it reaches a terminal state, with a live progress widget in TUI mode.</p> <pre><code>padwan-cli batch poll -j &lt;job-name&gt;\n\n# Custom interval and timeout\npadwan-cli batch poll -j &lt;job-name&gt; -i 10 -t 300\n\n# Save results to file on completion\npadwan-cli batch poll -j &lt;job-name&gt; -o results.json\n</code></pre> Option Default Description <code>-j</code>, <code>--job</code> required Batch job name <code>-i</code>, <code>--interval</code> <code>5</code> Poll interval in seconds <code>-t</code>, <code>--timeout</code> none Maximum wait time in seconds <code>-r</code>, <code>--results</code> <code>true</code> Show results on completion <code>-o</code>, <code>--output</code> Save results to file (format inferred from extension)"},{"location":"commands/batch/#batch-cancel","title":"<code>batch cancel</code>","text":"<p>Cancel a pending or running batch job.</p> <pre><code>padwan-cli batch cancel -j &lt;job-name&gt;\n</code></pre> Option Default Description <code>-j</code>, <code>--job</code> required Batch job name to cancel"},{"location":"commands/batch/#batch-retry","title":"<code>batch retry</code>","text":"<p>Identify failed requests from a completed batch and report them for retry.</p> <pre><code>padwan-cli batch retry -j &lt;job-name&gt;\n</code></pre> Option Default Description <code>-j</code>, <code>--job</code> required Original job name <p>The command inspects the original job's responses, identifies requests that returned errors or had no candidates, and lists their keys.</p> <p>Warning</p> <p>Retry requires the original prompts to be available. Currently this command reports failed keys but does not automatically re-submit them.</p>"},{"location":"commands/batch/#batch-export","title":"<code>batch export</code>","text":"<p>Export completed batch results to a file.</p> <pre><code>padwan-cli batch export -j &lt;job-name&gt; -o results.json\npadwan-cli batch export -j &lt;job-name&gt; -o results.csv -f csv\n</code></pre> Option Default Description <code>-j</code>, <code>--job</code> required Batch job name <code>-o</code>, <code>--output</code> required Output file path <code>-f</code>, <code>--format</code> <code>json</code> Output format: <code>json</code>, <code>csv</code>, or <code>txt</code>"},{"location":"commands/batch/#export-formats","title":"Export formats","text":"<ul> <li>json \u2014 structured array with <code>key</code>, <code>content</code>, <code>input_tokens</code>, <code>output_tokens</code>, <code>total_tokens</code></li> <li>csv \u2014 tabular with the same fields as columns</li> <li>txt \u2014 plain text with <code>[key]</code> headers followed by content</li> </ul>"},{"location":"commands/chat/","title":"Chat","text":"<p>The <code>chat</code> command group provides interactive, multi-turn conversations with LLMs. Sessions are persisted in memory per model for the lifetime of the process.</p> <p></p>"},{"location":"commands/chat/#chat-send","title":"<code>chat send</code>","text":"<p>Start (or continue) a conversation with a model.</p> <pre><code>padwan-cli chat send \"Hello, how are you?\" -m gpt-4o-mini\n</code></pre> Option Default Description <code>MESSAGE</code> (positional) required The message to send <code>-m</code>, <code>--model</code> <code>gpt-4o-mini</code> Model to use <p>In TUI mode, the command enters an interactive loop \u2014 type follow-up messages and press Enter to continue the conversation. Press Ctrl+C to exit chat mode. Messages you type while the model is responding are queued and processed in order.</p> <p>In CLI mode, the command sends a single message, prints the response, and exits.</p> <p>After each response, token usage is displayed:</p> <pre><code>in: 42 out: 128 cached: 0 | session: 170\n</code></pre> <ul> <li>in \u2014 input tokens for the last request</li> <li>out \u2014 output tokens for the last request</li> <li>cached \u2014 cached tokens (if supported by the provider)</li> <li>session \u2014 cumulative tokens across the conversation</li> </ul>"},{"location":"commands/chat/#session-persistence","title":"Session persistence","text":"<p>Each model gets its own conversation session. Sending messages to <code>gpt-4o-mini</code> and then to <code>gemini-2.0-flash</code> creates two independent sessions. Switching back to a model resumes where you left off.</p>"},{"location":"commands/chat/#chat-clear","title":"<code>chat clear</code>","text":"<p>Clear conversation history.</p> <pre><code># Clear history for a specific model\npadwan-cli chat clear -m gpt-4o-mini\n\n# Clear all sessions\npadwan-cli chat clear\n</code></pre> Option Default Description <code>-m</code>, <code>--model</code> all Model session to clear. Clears all sessions if omitted."}]}